{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a1b0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "import math\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torchvision \n",
    "import torch.utils.data as data\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f678c39",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d9330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('books_autorec.csv')\n",
    "df_books.sort_values(by='ratings_count', ascending=False, inplace=True)\n",
    "#for aumenting books number change \n",
    "Books_number = 2000\n",
    "df_books = df_books.iloc[:Books_number]\n",
    "df_books.to_csv('Spatial_model_books.csv')\n",
    "df_books['goodreads_book_id'] = df_books['goodreads_book_id'].astype(int)\n",
    "book_ids = df_books['goodreads_book_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e44f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('Spatial_model_books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34cad950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"books_autorec.csv\")\n",
    "df_ratings = pd.read_csv(\"ratings_autorec.csv\")\n",
    "\n",
    "df_ratings_with_clusters = df_ratings.merge(\n",
    "    df[['goodreads_book_id', 'cluster']], \n",
    "    left_on='book_id', \n",
    "    right_on='goodreads_book_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# create dictionary with user and ratings\n",
    "sparse_users = {}\n",
    "for user_id, group in df_ratings_with_clusters.groupby('user_id'):\n",
    "    books_ratings_clusters = group[['book_id', 'rating', 'cluster']].values.tolist()\n",
    "    sparse_users[user_id] = books_ratings_clusters\n",
    "\n",
    "cluster_sizes = df_books['cluster'].value_counts().sort_index().values\n",
    "\n",
    "filter_users = {\n",
    "    user: [triplet for triplet in triplets if triplet[0] in book_ids]\n",
    "    for user, triplets in sparse_users.items()\n",
    "}\n",
    "filter_users = {user: triplets for user, triplets in filter_users.items() if triplets}\n",
    "#user taken\n",
    "taken_users = 20000\n",
    "filter_users = sorted(filter_users.items(), key=lambda x: len(x[1]), reverse=True)[:taken_users]\n",
    "filter_users = dict(filter_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ac99d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53424\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len(sparse_users))\n",
    "print(len(filter_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33aeea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "mapping_pos_to_books = dict(zip(range(Books_number), book_ids))\n",
    "mapping_books_to_pos = dict(zip(book_ids,range(Books_number)))\n",
    "mapping_pos_to_users = dict(zip(range(taken_users), filter_users.keys()))\n",
    "mapping_users_to_pos = dict(zip(filter_users.keys(),range(taken_users)))\n",
    "print(len(mapping_pos_to_books))\n",
    "print(len(mapping_books_to_pos))\n",
    "print(len(mapping_pos_to_users))\n",
    "print(len(mapping_users_to_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750eb98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "2000\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "n_books = len(mapping_books_to_pos)\n",
    "user_vectors = []\n",
    "for user_id, triplets in filter_users.items():\n",
    "    vector = np.zeros(n_books)  # inizializza vettore di zeri\n",
    "\n",
    "    for book_id, rating, _ in triplets:\n",
    "        if book_id in mapping_books_to_pos:  # se il book_id è tra quelli mappati\n",
    "            index = mapping_books_to_pos[book_id]\n",
    "            vector[index] = rating  # inserisci il rating nella posizione giusta\n",
    "\n",
    "    user_vectors.append(vector)\n",
    "print(len(user_vectors))\n",
    "print(len(user_vectors[0]))\n",
    "for i in range(len(user_vectors)):\n",
    "    user_vectors[i] = [0 if elem < 3 else 1 for elem in user_vectors[i]]\n",
    "print(user_vectors[0])\n",
    "df_input_data = pd.DataFrame(user_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4873d3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spatial_F_AE(nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Spatial_F_AE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(k,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250,125),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(125,50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2,50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,125),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(125,250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,k),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        final = self.decoder(z)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spatial_F_AE2(nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Spatial_F_AE2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(k,750),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(750,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250,125),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(125,75),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(75,50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25,10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2,10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25,50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,75),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(75,125),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(125,250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,750),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(750,k),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        final = self.decoder(z)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85667552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, num_epochs, scheduler=None, best_loss=float('inf')):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    losses = []\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            inputs = batch[0]\n",
    "            inputs = inputs.to(device)\n",
    "            recon = model(inputs)\n",
    "            loss = criterion(recon, inputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(loss.item())\n",
    "            running_loss += loss.item()\n",
    "        losses.append(running_loss / (i + 1))\n",
    "        if running_loss < best_loss:\n",
    "            best_loss = running_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Epoch {epoch+1}: Loss = {running_loss / (i + 1):.10f}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_graph(tr_loss,n_epochs):\n",
    "    plt.plot(range(n_epochs),tr_loss,label='tr_loss', c='black')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c295695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_input_data = pd.read_csv('Spatial_model_inputs.csv')\n",
    "tensor_data = torch.tensor(df_input_data.values, dtype=torch.float32)\n",
    "dataset = torch.utils.data.TensorDataset(tensor_data)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d543211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteovicenzino/Documents/UNI/3_anno_24_25/ML/venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "model = Spatial_F_AE(Books_number)\n",
    "criterion = nn.MSELoss()\n",
    "N_Epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=False)\n",
    "\n",
    "losses = train(model,dataloader,criterion, optimizer, N_Epochs, scheduler)\n",
    "loss_graph(losses, N_Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Spatial_F_AE2(Books_number)\n",
    "criterion = nn.MSELoss()\n",
    "N_Epochs = 20\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr = 0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=False)\n",
    "\n",
    "losses = train(model,dataloader,criterion, optimizer, N_Epochs, scheduler)\n",
    "loss_graph(losses, N_Epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbb325",
   "metadata": {},
   "source": [
    "## Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(user, model):\n",
    "    model.eval()\n",
    "    recon = model(user)\n",
    "    old_books= (user != 0).nonzero(as_tuple=True)[0].tolist()\n",
    "    _, new_books = torch.topk(recon, 25)\n",
    "    new_books = new_books.tolist()\n",
    "    old_books_map = [mapping_pos_to_books[pos] for pos in old_books]\n",
    "    new_books_map = [mapping_pos_to_books[pos] for pos in new_books]\n",
    "    old_titles = [df_books[df_books['goodreads_book_id'] == id].values.tolist()[0][5] for id in old_books_map]\n",
    "    new_titles = [df_books[df_books['goodreads_book_id'] == id].values.tolist()[0][5] for id in new_books_map]\n",
    "    diff = list(set(new_titles) - set(old_titles))\n",
    "    diff_id = list(set(new_books_map) - set(old_books_map))\n",
    "    return diff, diff_id, old_books_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b87b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomendations for user 678: \n",
      "['A Tale of Two Cities', 'Memoirs of a Geisha', 'Angels & Demons ', ' The Fellowship of the Ring', 'Un di Velt Hot Geshvign', 'Pippi Långstrump', 'The Da Vinci Code', 'The Curious Incident of the Dog in the Night-Time', 'Modern Romance', 'Of Mice and Men ', 'High Five', 'Freakonomics: A Rogue Economist Explores the Hidden Side of Everything', 'A Confederacy of Dunces', 'Ὀδύσσεια', 'O Alquimista', 'The Tragicall Historie of Hamlet, Prince of Denmark', 'The Boston Girl', 'Beautiful Creatures']\n",
      "[960, 1953, 930, 34, 865, 1381, 19302, 968, 22450859, 1420, 6304335, 1617, 1202, 1618, 310612, 6423, 23453112, 890]\n"
     ]
    }
   ],
   "source": [
    "rec, rec_id, old_id = recommendations(dataloader.dataset[678][0], model)\n",
    "print(\"Recommendations for user 678: \")\n",
    "print(rec)\n",
    "print(rec_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babcdb3",
   "metadata": {},
   "source": [
    "Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a655688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tags_for_user(df, books_id, top_n=-1):\n",
    "    filtered_df = df[df['goodreads_book_id'].isin(books_id)]\n",
    "    tags = filtered_df['tags_list']\n",
    "    \n",
    "    all_tags = []\n",
    "    for t in tags:\n",
    "        if isinstance(t, str):\n",
    "            t = ast.literal_eval(t)\n",
    "        all_tags.extend(t)\n",
    "\n",
    "    tag_counts = Counter(all_tags)\n",
    "    if top_n == -1:\n",
    "        top_n = len(tag_counts)\n",
    "    most_common = tag_counts.most_common(top_n)\n",
    "    return most_common\n",
    "\n",
    "\n",
    "def top_tags_for_user(df, books_id, top_n=-1):\n",
    "    filtered_df = df[df['goodreads_book_id'].isin(books_id)]\n",
    "    tags = filtered_df['tags_list']\n",
    "    \n",
    "    all_tags = []\n",
    "    for t in tags:\n",
    "        # Only try to parse if it's a string and not already a list\n",
    "        if isinstance(t, str):\n",
    "            try:\n",
    "                t = ast.literal_eval(t)\n",
    "            except Exception:\n",
    "                # If parsing fails, skip this entry\n",
    "                continue\n",
    "        if isinstance(t, list):\n",
    "            all_tags.extend(t)\n",
    "        # If it's not a list, skip\n",
    "\n",
    "    tag_counts = Counter(all_tags)\n",
    "    if top_n == -1:\n",
    "        top_n = len(tag_counts)\n",
    "    most_common = tag_counts.most_common(top_n)\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "644e6fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Books [960, 1953, 1381, 19302, 22450859, 1420, 6304335, 1617, 1202, 310612, 6423, 23453112]:\n",
      "  fiction:  12\n",
      "  favorites:  12\n",
      "  owned:  12\n",
      "  books-i-own:  12\n",
      "  owned-books:  12\n",
      "  currently-reading:  12\n",
      "  library:  12\n",
      "  to-buy:  11\n",
      "  audiobook:  11\n",
      "  kindle:  11\n",
      "  audiobooks:  11\n",
      "  own-it:  11\n",
      "  audio:  11\n",
      "  default:  10\n",
      "  adult:  10\n",
      "  my-library:  10\n",
      "  i-own:  10\n",
      "  wish-list:  10\n",
      "  eng:  10\n",
      "  favourites:  9\n",
      "  my-books:  9\n",
      "  ebook:  9\n",
      "  ebooks:  9\n",
      "  contemporary:  8\n",
      "  literature:  8\n",
      "  book-club:  8\n",
      "  re-read:  8\n",
      "  novels:  8\n",
      "  adult-fiction:  7\n",
      "  books:  7\n",
      "  all-time-favorites:  7\n",
      "  classics:  7\n",
      "  romance:  7\n",
      "  abandoned:  7\n",
      "  series:  6\n",
      "  historical-fiction:  6\n",
      "  historical:  6\n",
      "  history:  6\n",
      "  borrowed:  6\n",
      "  shelfari-favorites:  6\n",
      "  favorite-books:  6\n",
      "  school:  6\n",
      "  classic:  6\n",
      "  unfinished:  6\n",
      "  audible:  6\n",
      "  novel:  5\n",
      "  general-fiction:  5\n",
      "  fantasy:  5\n",
      "  english:  5\n",
      "  favorite:  5\n",
      "  american:  5\n",
      "  home-library:  5\n",
      "  high-school:  5\n",
      "  to-re-read:  5\n",
      "  did-not-finish:  5\n",
      "  didn-t-finish:  5\n",
      "  audio-book:  5\n",
      "  audio-books:  5\n",
      "  humor:  5\n",
      "  adventure:  4\n",
      "  contemporary-fiction:  4\n",
      "  drama:  4\n",
      "  finished:  4\n",
      "  other:  4\n",
      "  non-fiction:  4\n",
      "  read-for-school:  4\n",
      "  for-school:  4\n",
      "  school-books:  4\n",
      "  young-adult:  4\n",
      "  school-reads:  4\n",
      "  rory-gilmore-reading-challenge:  4\n",
      "  required-reading:  4\n",
      "  read-in-school:  4\n",
      "  read-in-2016:  4\n",
      "  rory-gilmore-challenge:  4\n",
      "  read-in-2014:  4\n",
      "  read-in-2015:  4\n",
      "  classic-literature:  4\n",
      "  classics-to-read:  4\n",
      "  literary:  4\n",
      "  literary-fiction:  4\n",
      "  lit:  4\n",
      "  e-book:  4\n",
      "  bookclub:  4\n",
      "  recommended:  4\n",
      "  library-books:  4\n",
      "  humour:  4\n",
      "  funny:  4\n",
      "  mystery:  3\n",
      "  5-stars:  3\n",
      "  nonfiction:  3\n",
      "  memoir:  3\n",
      "  europe:  3\n",
      "  school-reading:  3\n",
      "  war:  3\n",
      "  ya:  3\n",
      "  books-for-school:  3\n",
      "  read-in-2017:  3\n",
      "  rory-gilmore:  3\n",
      "  read-in-2013:  3\n",
      "  philosophy:  3\n",
      "  classic-lit:  3\n",
      "  classic-fiction:  3\n",
      "  the-classics:  3\n",
      "  1001-books:  3\n",
      "  1001:  3\n",
      "  e-books:  3\n",
      "  1001-books-to-read-before-you-die:  3\n",
      "  nook:  3\n",
      "  to-read-fiction:  3\n",
      "  science:  3\n",
      "  dnf:  3\n",
      "  comedy:  3\n",
      "  20th-century:  3\n",
      "  library-book:  3\n",
      "  thriller:  2\n",
      "  suspense:  2\n",
      "  mystery-thriller:  2\n",
      "  crime:  2\n",
      "  religion:  2\n",
      "  mystery-suspense:  2\n",
      "  action:  2\n",
      "  mysteries:  2\n",
      "  crime-mystery:  2\n",
      "  mystery-crime:  2\n",
      "  modern-fiction:  2\n",
      "  detective:  2\n",
      "  american-literature:  2\n",
      "  paperback:  2\n",
      "  murder-mystery:  2\n",
      "  bookshelf:  2\n",
      "  روايات:  2\n",
      "  2006:  2\n",
      "  have:  2\n",
      "  movie:  2\n",
      "  personal-library:  2\n",
      "  modern:  2\n",
      "  memoirs:  2\n",
      "  autobiography:  2\n",
      "  biography-memoir:  2\n",
      "  jewish:  2\n",
      "  biographies:  2\n",
      "  biography:  2\n",
      "  judaism:  2\n",
      "  read-again:  2\n",
      "  death:  2\n",
      "  reread:  2\n",
      "  poetry:  2\n",
      "  college:  2\n",
      "  to-read-classics:  2\n",
      "  read-in-high-school:  2\n",
      "  university:  2\n",
      "  plays:  2\n",
      "  must-read:  2\n",
      "  fiction-classics:  2\n",
      "  british:  2\n",
      "  my-ebooks:  2\n",
      "  british-literature:  2\n",
      "  england:  2\n",
      "  english-literature:  2\n",
      "  kindle-books:  2\n",
      "  british-lit:  2\n",
      "  tbr:  2\n",
      "  brit-lit:  2\n",
      "  on-hold:  2\n",
      "  A Tale of Two Cities:  2\n",
      "  psychology:  2\n",
      "  sociology:  2\n",
      "  social-science:  2\n",
      "  culture:  2\n",
      "  non-fic:  2\n",
      "  to-read-non-fiction:  2\n",
      "  essays:  2\n",
      "  to-read-nonfiction:  2\n",
      "  general:  2\n",
      "  non-fiction-to-read:  2\n",
      "  self-help:  2\n",
      "  nonfic:  2\n",
      "  21st-century:  2\n",
      "  adult-nonfiction:  2\n",
      "  adult-non-fiction:  2\n",
      "  read-more-than-once:  2\n",
      "  4-stars:  2\n",
      "  read-in-2012:  2\n",
      "  read-in-2010:  2\n",
      "  love:  2\n",
      "  gave-up-on:  2\n",
      "  read-in-2011:  2\n",
      "  maybe:  2\n",
      "  southern:  2\n",
      "  want-to-read:  2\n",
      "  couldn-t-finish:  2\n",
      "  humorous:  2\n",
      "  never-finished:  2\n",
      "  new:  2\n",
      "  1001-books-to-read:  2\n",
      "  A Confederacy of Dunces:  2\n",
      "  friendship:  2\n",
      "  relationships:  2\n",
      "  read-2015:  2\n",
      "  read-2016:  2\n",
      "  2016-reading-challenge:  2\n",
      "  2016-reads:  2\n",
      "  listened-to:  2\n",
      "  2015-reads:  2\n",
      "  2016-books:  2\n",
      "  2015-books:  2\n",
      "  read-2017:  2\n",
      "  2016-read:  2\n",
      "  2017-reads:  2\n",
      "  2015-read:  2\n",
      "  summer-2015:  2\n",
      "  books-read-in-2015:  2\n",
      "  fun:  2\n",
      "  overdrive:  2\n",
      "  Modern Romance:  2\n",
      "  chick-lit:  2\n",
      "  women:  2\n",
      "  The Boston Girl:  2\n",
      "  dan-brown:  1\n",
      "  thrillers:  1\n",
      "  conspiracy:  1\n",
      "  action-adventure:  1\n",
      "  suspense-thriller:  1\n",
      "  robert-langdon:  1\n",
      "  crime-thriller:  1\n",
      "  thriller-mystery:  1\n",
      "  italy:  1\n",
      "  science-fiction:  1\n",
      "  brown-dan:  1\n",
      "  thriller-suspense:  1\n",
      "  sci-fi:  1\n",
      "  brown:  1\n",
      "  religious:  1\n",
      "  movies:  1\n",
      "  popular-fiction:  1\n",
      "  2005:  1\n",
      "  mystery-thrillers:  1\n",
      "  mistery:  1\n",
      "  read-fiction:  1\n",
      "  art:  1\n",
      "  favourite:  1\n",
      "  realistic-fiction:  1\n",
      "  Dan Brown:  1\n",
      "  Angels & Demons :  1\n",
      "  Angels & Demons  (Robert Langdon, #1):  1\n",
      "  en-CA:  1\n",
      "  2000.0:  1\n",
      "  wwii:  1\n",
      "  world-war-ii:  1\n",
      "  ww2:  1\n",
      "  memoir-biography:  1\n",
      "  world-war-2:  1\n",
      "  holocaust:  1\n",
      "  survival:  1\n",
      "  biographies-memoirs:  1\n",
      "  biography-autobiography:  1\n",
      "  biographical:  1\n",
      "  oprah-s-book-club:  1\n",
      "  oprah:  1\n",
      "  germany:  1\n",
      "  genocide:  1\n",
      "  autobiography-memoir:  1\n",
      "  middle-school:  1\n",
      "  elie-wiesel:  1\n",
      "  bio:  1\n",
      "  made-me-cry:  1\n",
      "  night:  1\n",
      "  bio-memoir:  1\n",
      "  horror:  1\n",
      "  my-favorites:  1\n",
      "  historical-non-fiction:  1\n",
      "  Elie Wiesel, Marion Wiesel:  1\n",
      "  Un di Velt Hot Geshvign:  1\n",
      "  Night (The Night Trilogy #1):  1\n",
      "  1958.0:  1\n",
      "  mythology:  1\n",
      "  epic:  1\n",
      "  greek:  1\n",
      "  classical:  1\n",
      "  greece:  1\n",
      "  ancient:  1\n",
      "  ancient-greece:  1\n",
      "  epics:  1\n",
      "  greek-mythology:  1\n",
      "  epic-poetry:  1\n",
      "  homer:  1\n",
      "  antiquity:  1\n",
      "  translated:  1\n",
      "  greek-literature:  1\n",
      "  classical-literature:  1\n",
      "  myth:  1\n",
      "  9th-grade:  1\n",
      "  translation:  1\n",
      "  ancient-literature:  1\n",
      "  ancient-history:  1\n",
      "  to-reread:  1\n",
      "  travel:  1\n",
      "  great-books:  1\n",
      "  ancient-classics:  1\n",
      "  ancient-greek:  1\n",
      "  classical-studies:  1\n",
      "  Homer, Robert Fagles, E.V. Rieu, Frédéric Mugler, Bernard Knox:  1\n",
      "  Ὀδύσσεια:  1\n",
      "  The Odyssey:  1\n",
      "  -720.0:  1\n",
      "  charles-dickens:  1\n",
      "  19th-century:  1\n",
      "  dickens:  1\n",
      "  france:  1\n",
      "  victorian:  1\n",
      "  french-revolution:  1\n",
      "  paris:  1\n",
      "  own-to-read:  1\n",
      "  london:  1\n",
      "  Charles Dickens, Richard Maxwell, Hablot Knight Browne:  1\n",
      "  1859.0:  1\n",
      "  economics:  1\n",
      "  politics:  1\n",
      "  finance:  1\n",
      "  economy:  1\n",
      "  business:  1\n",
      "  behavioral-economics:  1\n",
      "  statistics:  1\n",
      "  society:  1\n",
      "  political:  1\n",
      "  econ:  1\n",
      "  business-books:  1\n",
      "  business-economics:  1\n",
      "  social:  1\n",
      "  education:  1\n",
      "  educational:  1\n",
      "  social-sciences:  1\n",
      "  mathematics:  1\n",
      "  behavioral:  1\n",
      "  pop-science:  1\n",
      "  reference:  1\n",
      "  general-non-fiction:  1\n",
      "  management:  1\n",
      "  popular:  1\n",
      "  social-commentary:  1\n",
      "  academic:  1\n",
      "  interesting:  1\n",
      "  misc:  1\n",
      "  thought-provoking:  1\n",
      "  current-events:  1\n",
      "  economic:  1\n",
      "  money:  1\n",
      "  read-non-fiction:  1\n",
      "  other-non-fiction:  1\n",
      "  current-affairs:  1\n",
      "  economics-business:  1\n",
      "  finance-economics:  1\n",
      "  Steven D. Levitt, Stephen J. Dubner:  1\n",
      "  Freakonomics: A Rogue Economist Explores the Hidden Side of Everything:  1\n",
      "  Freakonomics: A Rogue Economist Explores the Hidden Side of Everything (Freakonomics, #1):  1\n",
      "  en-US:  1\n",
      "  2005.0:  1\n",
      "  shakespeare:  1\n",
      "  play:  1\n",
      "  theatre:  1\n",
      "  tragedy:  1\n",
      "  theater:  1\n",
      "  teatro:  1\n",
      "  17th-century:  1\n",
      "  william-shakespeare:  1\n",
      "  ghosts:  1\n",
      "  renaissance:  1\n",
      "  denmark:  1\n",
      "  assigned-reading:  1\n",
      "  read-in-english:  1\n",
      "  murder:  1\n",
      "  ap-lit:  1\n",
      "  uni:  1\n",
      "  for-class:  1\n",
      "  english-lit:  1\n",
      "  hamlet:  1\n",
      "  revenge:  1\n",
      "  classici:  1\n",
      "  clásicos:  1\n",
      "  classics-read:  1\n",
      "  William Shakespeare, Richard Andrews, Rex Gibson:  1\n",
      "  The Tragicall Historie of Hamlet, Prince of Denmark:  1\n",
      "  Hamlet:  1\n",
      "  1600.0:  1\n",
      "  paranormal:  1\n",
      "  magic:  1\n",
      "  supernatural:  1\n",
      "  witches:  1\n",
      "  paranormal-romance:  1\n",
      "  urban-fantasy:  1\n",
      "  teen:  1\n",
      "  caster-chronicles:  1\n",
      "  ya-fantasy:  1\n",
      "  read-2013:  1\n",
      "  ya-fiction:  1\n",
      "  beautiful-creatures:  1\n",
      "  kami-garcia:  1\n",
      "  gothic:  1\n",
      "  ya-paranormal:  1\n",
      "  ya-books:  1\n",
      "  2013-reads:  1\n",
      "  young-adult-fiction:  1\n",
      "  sci-fi-fantasy:  1\n",
      "  meh:  1\n",
      "  first-in-series:  1\n",
      "  book-to-movie:  1\n",
      "  reviewed:  1\n",
      "  teen-fiction:  1\n",
      "  male-pov:  1\n",
      "  books-i-have:  1\n",
      "  ya-lit:  1\n",
      "  fantasy-sci-fi:  1\n",
      "  my-bookshelf:  1\n",
      "  signed:  1\n",
      "  2013-books:  1\n",
      "  fantasy-paranormal:  1\n",
      "  paranormal-fantasy:  1\n",
      "  bought:  1\n",
      "  3-stars:  1\n",
      "  ya-romance:  1\n",
      "  lost-interest:  1\n",
      "  Kami Garcia, Margaret Stohl:  1\n",
      "  Beautiful Creatures:  1\n",
      "  Beautiful Creatures (Caster Chronicles, #1):  1\n",
      "  2009.0:  1\n",
      "  pulitzer:  1\n",
      "  pulitzer-prize:  1\n",
      "  new-orleans:  1\n",
      "  satire:  1\n",
      "  pulitzer-prize-winners:  1\n",
      "  pulitzer-winners:  1\n",
      "  pulitzers:  1\n",
      "  american-lit:  1\n",
      "  usa:  1\n",
      "  gilmore-girls:  1\n",
      "  rory-gilmore-reading-list:  1\n",
      "  southern-lit:  1\n",
      "  pulitzer-fiction:  1\n",
      "  1001-books-you-must-read-before-you:  1\n",
      "  modern-classics:  1\n",
      "  gave-up:  1\n",
      "  could-not-finish:  1\n",
      "  louisiana:  1\n",
      "  southern-literature:  1\n",
      "  pulitzer-prize-fiction:  1\n",
      "  novela:  1\n",
      "  orleans:  1\n",
      "  america:  1\n",
      "  american-fiction:  1\n",
      "  pulitzer-prize-winner:  1\n",
      "  John Kennedy Toole, Walker Percy:  1\n",
      "  1980.0:  1\n",
      "  childrens:  1\n",
      "  children:  1\n",
      "  children-s:  1\n",
      "  children-s-books:  1\n",
      "  childhood:  1\n",
      "  kids:  1\n",
      "  childrens-books:  1\n",
      "  kids-books:  1\n",
      "  childhood-favorites:  1\n",
      "  middle-grade:  1\n",
      "  childhood-books:  1\n",
      "  children-s-literature:  1\n",
      "  children-s-lit:  1\n",
      "  children-books:  1\n",
      "  juvenile:  1\n",
      "  swedish:  1\n",
      "  read-aloud:  1\n",
      "  childrens-lit:  1\n",
      "  read-alouds:  1\n",
      "  chapter-books:  1\n",
      "  scandinavian:  1\n",
      "  youth:  1\n",
      "  childhood-reads:  1\n",
      "  juvenile-fiction:  1\n",
      "  childrens-literature:  1\n",
      "  sweden:  1\n",
      "  childhood-favourites:  1\n",
      "  my-childhood:  1\n",
      "  read-as-a-child:  1\n",
      "  children-s-fiction:  1\n",
      "  kid-lit:  1\n",
      "  read-as-a-kid:  1\n",
      "  kid-books:  1\n",
      "  childrens-classics:  1\n",
      "  astrid-lindgren:  1\n",
      "  childhood-memories:  1\n",
      "  for-the-kids:  1\n",
      "  children-young-adult:  1\n",
      "  childrens-fiction:  1\n",
      "  children-s-classics:  1\n",
      "  children-ya:  1\n",
      "  elementary:  1\n",
      "  books-from-my-childhood:  1\n",
      "  childhood-favs:  1\n",
      "  read-with-kids:  1\n",
      "  scandinavia:  1\n",
      "  children-s-book:  1\n",
      "  nostalgia:  1\n",
      "  young-readers:  1\n",
      "  for-kids:  1\n",
      "  children-s-chapter-books:  1\n",
      "  family-read-alouds:  1\n",
      "  swedish-literature:  1\n",
      "  female-author:  1\n",
      "  1001-import:  1\n",
      "  childhood-faves:  1\n",
      "  childhood-favorite:  1\n",
      "  kidlit:  1\n",
      "  Astrid Lindgren, Lauren Child, Florence Lamborn, Nancy Seligsohn:  1\n",
      "  Pippi Långstrump:  1\n",
      "  Pippi Longstocking:  1\n",
      "  1945.0:  1\n",
      "  listened:  1\n",
      "  dating:  1\n",
      "  books-read-in-2016:  1\n",
      "  pop-culture:  1\n",
      "  technology:  1\n",
      "  2016-challenge:  1\n",
      "  celebrity:  1\n",
      "  summer-2016:  1\n",
      "  sexuality:  1\n",
      "  comedians:  1\n",
      "  2017-reading-challenge:  1\n",
      "  poc-author:  1\n",
      "  Aziz Ansari, Eric Klinenberg:  1\n",
      "  2015.0:  1\n",
      "  janet-evanovich:  1\n",
      "  stephanie-plum:  1\n",
      "  evanovich:  1\n",
      "  stephanie-plum-series:  1\n",
      "  evanovich-janet:  1\n",
      "  stéphanie-plum:  1\n",
      "  plum:  1\n",
      "  cozy-mystery:  1\n",
      "  chicklit:  1\n",
      "  bounty-hunter:  1\n",
      "  stephanie:  1\n",
      "  chic-lit:  1\n",
      "  contemporary-romance:  1\n",
      "  crime-fiction:  1\n",
      "  fluff:  1\n",
      "  cozy-mysteries:  1\n",
      "  guilty-pleasures:  1\n",
      "  laugh-out-loud:  1\n",
      "  hilarious:  1\n",
      "  new-jersey:  1\n",
      "  detectives:  1\n",
      "  plum-series:  1\n",
      "  bounty:  1\n",
      "  read-2012:  1\n",
      "  mystery-series:  1\n",
      "  jersey:  1\n",
      "  read-2011:  1\n",
      "  just-for-fun:  1\n",
      "  janet-evanovich-stephanie-plum:  1\n",
      "  brain-candy:  1\n",
      "  series-stephanie-plum:  1\n",
      "  favorite-series:  1\n",
      "  trenton:  1\n",
      "  mystery-detective:  1\n",
      "  sleuths:  1\n",
      "  hunters:  1\n",
      "  Janet Evanovich:  1\n",
      "  High Five:  1\n",
      "  High Five (Stephanie Plum, #5):  1\n",
      "  1999.0:  1\n",
      "  boston:  1\n",
      "  coming-of-age:  1\n",
      "  book-club-books:  1\n",
      "  family:  1\n",
      "  book-group:  1\n",
      "  book-club-reads:  1\n",
      "  feminism:  1\n",
      "  new-england:  1\n",
      "  anita-diamant:  1\n",
      "  netgalley:  1\n",
      "  family-relationships:  1\n",
      "  2015-reading-challenge:  1\n",
      "  massachusetts:  1\n",
      "  immigrant:  1\n",
      "  immigrants:  1\n",
      "  2015-books-read:  1\n",
      "  books-read-2015:  1\n",
      "  the-boston-girl:  1\n",
      "  first-reads:  1\n",
      "  2015-challenge:  1\n",
      "  book-club-book:  1\n",
      "  book-club-selection:  1\n",
      "  women-s-fiction:  1\n",
      "  family-saga:  1\n",
      "  adult-historical-fiction:  1\n",
      "  best-of-2015:  1\n",
      "  2017-books:  1\n",
      "  Anita Diamant:  1\n",
      "  2014.0:  1\n"
     ]
    }
   ],
   "source": [
    "tags_recomendation = top_tags_for_user(df_books, rec_id)\n",
    "\n",
    "print(f\"\\nBooks {rec_id}:\")\n",
    "for tag, count in tags_recomendation:\n",
    "    print(f\"  {tag}:  {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_different_tags(tags_user_preference, tags_recomendation):\n",
    "    user_tags = set(tag for tag, _ in tags_user_preference)\n",
    "    rec_tags = set(tag for tag, _ in tags_recomendation)\n",
    "    diff_tags = rec_tags - user_tags\n",
    "    if len(rec_tags) == 0:\n",
    "        return 0.0\n",
    "    percent_diff = len(diff_tags) / len(rec_tags) * 100\n",
    "    return percent_diff\n",
    "\n",
    "def evaluate_user_percentage(user, model):\n",
    "    # get recommendations\n",
    "    _, rec_id, old_id = recommendations(dataloader.dataset[user][0], model)\n",
    "    tags_user_preference = top_tags_for_user(df_books, old_id)\n",
    "    tags_recomendation = top_tags_for_user(df_books, rec_id)\n",
    "    \n",
    "    # get random books\n",
    "    n_recom_books = len(rec_id)\n",
    "    sample_books = df_books.sample(n=n_recom_books)['goodreads_book_id'].tolist()\n",
    "    # get tags for recommended and random books \n",
    "    tags_random_books = top_tags_for_user(df_books, sample_books)\n",
    "    recommended = percentage_different_tags(tags_user_preference, tags_recomendation)\n",
    "    random = percentage_different_tags(tags_user_preference, tags_random_books)\n",
    "    return recommended, random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22711c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating user 678:\n",
      "Percentage of different tags in recommendations: 65.78%\n",
      "Percentage of different tags in random books: 69.62%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating user 678:\")\n",
    "recommended, random = evaluate_user_percentage(678, model)\n",
    "print(f\"Percentage of different tags in recommendations: {recommended:.2f}%\")\n",
    "print(f\"Percentage of different tags in random books: {random:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b099b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.43575163446907"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentages = []\n",
    "for user in range(len(dataloader.dataset)):\n",
    "    recommended, random = evaluate_user_percentage(user, model)\n",
    "    percentages.append(recommended - random)\n",
    "np.mean(percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53567212",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9180f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
